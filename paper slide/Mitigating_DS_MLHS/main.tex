\documentclass[paper slide]{beamer}
\usetheme{Boadilla}
\usepackage{essay-def}
\usepackage{bm}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{comment}
\usepackage{subcaption}
\usepackage{geometry}
\usepackage{algorithmic}
\usepackage{algpseudocode}
\usepackage{algorithmicx}
\geometry{left=1cm,right=1cm}
    \title[Mitigating DS in SciML]{Mitigating distribution shift in machine learning-augmented hybrid simulation\footnotemark}
\author[J. Zhao]{Jiaxi Zhao (NUS) \\ \small joint work with Prof. Qianxiao Li (NUS)}
\date[\today]{SciCADE2024 @ Singapore \\ \today}
\begin{document}
\par \setlength{\parindent}{2em}

\begin{frame}
\titlepage
\footnotetext[1]{https://arxiv.org/abs/2401.09259}
\end{frame}


\begin{frame}{Practical scientific problems}
	%#NOTE: try to explain to the audience that these two problems are very important and subject to a lot improvment based on data-driven model
	\begin{figure}[ht]
		\centering
		\begin{subfigure}{0.5\linewidth} % Adjust the width as needed
			\centering
			\includegraphics[width=\linewidth]{fig/urban_environment.jpeg}
			\caption{Computational fluid dynamics for urban environment}
		  \end{subfigure}%
		  \begin{subfigure}{0.5\linewidth} % Adjust the width as needed
			\centering
			\includegraphics[width=\linewidth]{fig/quantum_chemisty.png}
			\caption{Quantum chemistry for material science}
		  \end{subfigure}
	\end{figure}
\end{frame}


\begin{frame}{Data-driven scientific computing}
	\textbf{What are the problems we are interested in?}
	\begin{itemize}
		\item 1. {\color{red}Forward problem: Increase the stability and accuracy of machine learning-augmented simulation.}
		\item 2. Inverse problem: Learn the surrogate models and Perform effective sensitivity analysis to do inverse designs. 
	\end{itemize}

	\textbf{What are methods we focus on?}
	\begin{itemize}
		\item 1. 100\% data-driven: AlphaFold series, FermiNet, Fourier neural operator, DeepONet.
		\item 2. {\color{red}50 \% Numerical + 50 \% data-driven: Machine learning turbulence modeling, force fields, exchange-correlation functionals.}
	\end{itemize}
\end{frame}

\begin{frame}{A Framework for the hybrid approach}
	Simulating the dynamics:
	\bequn
		\begin{aligned}
			\p_t \mfx & = \mcL(\mfx, \mfu, t), \quad \mfx \in \mcX, \mfu \in \mcU, \mcL: \mcX \times \mcU \times \mbR_+ \rightarrow T\mcX,			\\
			\mfu & = \phi(\mfx, t), \quad \phi: \mcX \times \mbR_+ \rightarrow \mfu.
		\end{aligned}
	\eequn
	\begin{itemize}
		\item 1. $\mcL$ is known, possibly non-linear.
		\item 2. $\phi$ is un-known.
		\item 3. A set of data pairs $\lbb (\mfx_1, \mfu_1, t_1), (\mfx_2, \mfu_2, t_2), \cdots, (\mfx_N, \mfu_N, t_N )\rbb. $
		\item 4. {\color{red}Benchmark algorithm solves the ordinary least square:
		\bequn
			\arg\min_{\theta} \mbE \norml \mfu - \phi_{\theta}(\mfx, t) \normr^2.
		\eequn}
	\end{itemize}
		{\color{red}Why surrogate models? The dynamics are unknown, nonlinear, computationally expensive etc.}
\end{frame}

\begin{frame}{An example}
	Let us use the incompressible NS equation as an example
	\begin{equation*}
    \begin{aligned}
        	\frac{\p \mfu}{\p t} + (\mfu \cdot \nabla)\mfu -  \nu \Delta \mfu & =   \nabla p, \quad T \in [0, 1], 	\\
		\nabla \cdot \mfu & = 0.
    \end{aligned}
\end{equation*}
Consider solving it using the projection method, in each step, we need to solve the following equation
\bequn
\begin{aligned}
	\mfu_{k+1} 	& = \mfu_k +
	\Delta t (\nu \Delta \mfu_k
	- (\mfu_k \cdot \nabla)\mfu_k - \nabla p_{k}),    \\
	p_{k} & = \phi(\mfu_k) = \Delta^{-1}(\nabla \cdot \lp \nu \Delta \mfu_k
	- (\mfu_k \cdot \nabla)\mfu_k\rp),   \\
\end{aligned}
\eequn

The most important features are: 
\begin{itemize}
	\item 1. {\color{red}iterative solver}
	\item 2. {\color{red}data-driven}
\end{itemize}
\end{frame}

\begin{frame}{Dilemma of data-driven scientific computing}
	In data-driven scientific computing, \textcolor{red}{dynamics itself} can cause \textcolor{red}{distribution mismatch} between the training and testing data.
	Similarly to the \textcolor{red}{extrapolation, out-of-distribution} issue in NLP.
	\begin{figure}[ht]
		\centering
			\centering
			\includegraphics[width=1.1\linewidth]{fig/dilemma.jpg}
	\end{figure}
\end{frame}

\begin{frame}{Comparison with classical numerical stability}
	This is different from the classical numerical stability issue. For the ablation study, we add {\color{red}noises of the same scale to the ground truth}
	(without the data-driven part) and compare the simulations.
	\begin{equation*}
		\begin{aligned}
			\text{Data-driven model:} \quad & \phi_{\theta},	\\
			\text{Perturbed ground truth:} \quad & \phi_* + \mbE\norml \mfu - \phi_{\theta}(\mfx, t) \normr \epsilon, \ \epsilon \sim \mcN(\mathbf{0}, \mfI)
		\end{aligned}
	\end{equation*}
	where $\phi_*$ can be replaced by $\phi^{\Delta x}$, a fine-grid numerical solver.
\end{frame}

\begin{frame}{Comparison with classical numerical stability}
	\begin{figure}[ht]
		\centering
			\centering
			\includegraphics[width=.8\linewidth]{fig/RD-ds.pdf}
	\end{figure}
\end{frame}

\begin{frame}{An heuristic solution}
	\begin{figure}[H]
		\centering
		\centerline{\includegraphics[width=0.8\linewidth]{fig/mfd.png}}
	  \end{figure}
	  We design an algorithm that {\color{red}favors $\mfu_2$ than $\mfu_1$} by adding some regularization.
	  This is very similar to the {\color{red}Dirac-Frenkel variational principle} in dynamical low-rank approximation.
	\end{frame}

\begin{frame}{Linear dynamics}
	We consider the following the \text{\color{red}linear} hybrid simulation problem
	\bequn
		\begin{aligned}
			\frac{d\mfu}{dt} & = A\mfu + B\mfy,  \quad \mfu\in\mbR^m, \mfy \in \mbR^n, A\in \mbR^{m \times m}, B\in \mbR^{m \times n}   \\
			\mfy & = C^* \mfu,  \quad C^* \in \mbR^{n\times m}.
		\end{aligned}
	\eequn
	The least squares estimator aims to minimize the following loss
	\begin{equation*}
		l_{\text{OLS}}(\wht C) = \mbE\norml (\wht C - C^*) \mfu \normr^2,
	\end{equation*}
	while the proposed estimator minimizes
	\begin{equation*}
		l_{\text{TR}}(C) := \mbE_{(\mfu, \mfy)} \lp \norml (\wht C - C^*) \mfu\normr_2^2 + \lambda \norml P_{V^{\perp}}(A + BC)\mfu \normr_2^2 \rp,
	\end{equation*}
\end{frame}

\begin{frame}{Linear theory}
	The tangent-space regularized estimator {\color{red}performs a weighted least squares}
	\begin{Prop}
		Given a data matrix $\mfU \in \mbR^{m \times N}$ with observation noise $\epsilon$ of the same shape 
		\begin{equation*}\label{estimator-formula}
			\begin{aligned}
			\wht C_{\text{OLS}} = & \ C^* P_V + \epsilon \mfU^{\dagger},    \\
			\wht C_{\text{TR}} = & \ (\mfI + \lambda B^T P_{V^{\perp}} B)^{-1}(C^* P_V + \epsilon \mfU^{\dagger} - \lambda B^T P_{V^{\perp}} AP_{V}).
			\end{aligned}
		\end{equation*}
	\end{Prop}
	Specifically, in the noiseless scenarios, we have
	\begin{equation*}
		\wht C_{\text{OLS}} = & \ C^* P_V, \quad \wht C_{\text{TR}} = (\mfI + \lambda P_{V^{\perp}})^{-1}(C^* P_V + \epsilon \mfU^{\dagger})= (\mfI + \lambda P_{V^{\perp}})^{-1}\wht C_{\text{OLS}}.
	\end{equation*}
\end{frame}

\begin{frame}{Linear theory}
	The tangent-space regularized estimator has {\color{red}slower error scaling for large $\lambda$}:
	%#TODO: still need to think about how to present this part
	\begin{Thm}
		With $Q_m(r, T)$ defined by $m^2\int_0^T (2 + t^{m-1})e^{r t}dt$ and
		\begin{equation*}
			\begin{aligned}
				e_1 = \text{eig}_{\max}(A+B\wht C), \quad e_2 = \text{eig}_{\max}((A+B\wht C)P_V), \\
				\text{eig}_{\max}(A) = \max\{\Re(s): \exists v \neq \mathbf{0}, Av = sv\},
			\end{aligned}
		\end{equation*}
		the errors of OLS and our algorithm are bounded respectively by
		\begin{equation*}\label{equ:a-posterior-error}
			\begin{aligned}
				& \ \mbE\norml \wht\mfu_{\text{OLS}}(T) - \mfu(T) \normr \leq c_1\sqrt{\delta}\norml B \normr_2Q_m(e_1, T),      \\
				& \ \mbE\norml \wht\mfu_{\text{TR}}(T) - \mfu(T) \normr \leq c_2\sqrt{\delta}\Big(\norml B \normr_2 Q_m(e_2, T) \\
			& + \frac{9m^4c_3}{\sqrt{\lambda}}\lp 1 + 3m^2c_1\sqrt{\delta}\norml B \normr_2\rp\norml A+B\wht C \normr_2 (1\vee T^{3m})(1\vee e^{e_1T}) \Big).
			\end{aligned}
		\end{equation*}
	\end{Thm}
\end{frame}

\begin{frame}{Linear experiments}
	Consider a linear synthetic dynamics where the data subspace $V(V^{\perp})$ is the stable (unstable) manifold.
	\begin{figure}[ht]\label{fig:linear-cmp}
		\centering
		\centerline{\includegraphics[width=.8\linewidth]{fig/exp2-1.pdf}}
	\end{figure}
\end{frame}

\begin{frame}{Nonlinear dynamics}
	\textcolor{red}{Use an autoencoder to parametrize the nonlinear data manifold.} We choose U-net\footnotemark as our autoencoder architecture trained via $\min_{D, E}\mbE\norml \mfu - D(E(\mfu))\normr^2$.
	Moreover, one can use $F(\mfu) = \norml \mfu - D(E(\mfu))\normr^2$ to quantify the distribution shift and $\nabla F(\mfu)$ as 
	{\color{red} a normal vector}! But there is a small caveat.
	\begin{figure}[H]
          \centering
          \centerline{\includegraphics[width=1.1\linewidth]{fig/Unet.png}}
\end{figure}
\footnotetext{Thuerey, Nils, et al. "Deep learning methods for Reynolds-averaged Navierâ€“Stokes simulations of airfoil flows." AIAA Journal 58.1 (2020): 25-36.}
\end{frame}

\begin{frame}{Overall algorithm}
	%#TODO: the algorithm itself is hard to present, think about how to simplify it
	\begin{figure}
		\centering
		\centerline{\includegraphics[width=\linewidth]{fig/alg.jpg}}
	  \end{figure}
\end{frame}

\begin{frame}{Performance comparison}
	\begin{figure}[H]
          \centering
          \centerline{\includegraphics[width=.75\linewidth]{fig/RD-TR.pdf}}
\end{figure}
\end{frame}

\begin{frame}{Performance comparison}
	\begin{figure}[H]
          \centering
          \centerline{\includegraphics[width=.75\linewidth]{fig/NS-TR.pdf}}
          \caption{Comparison of our method and naive method}
\end{figure}
\end{frame}

\begin{frame}{Future on-going work}
	\begin{itemize}
		\item 1. Deploy to practical problems: Subgrid-scale modeling in large eddy simulation of the
		urban environment.
		\begin{equation*}
			\begin{aligned}
			\frac{\p \overline{u}_i}{\p t} + \frac{\p}{\p x_j}(\overline{u}_i\overline{u}_j) & \ = -\frac{\p \overline{p}}{\p x_i} + \nu\Delta \overline{u}_i - \frac{\p \tau_{ij}}{\p x_j},   \\
			\frac{\p \overline{u}_i}{\p x_i} & \ = 0.
			\end{aligned}
		  \end{equation*}
		\item 2. Extend the framework to solve the inverse problem via surrogate modeling.
		\item 3. Theoretical analysis of the proposed framework, a.k.a. numerical analysis for SciML.
	\end{itemize}
\end{frame}

\begin{frame}{Reaction-diffusion equation}
	Consider the following FitzHugh-Nagumo reaction-diffusion equation:
	\begin{equation}
    \begin{aligned}
        	\frac{\p \mfu}{\p t} & = \gamma \Delta \mfu + \mfR(\mfu), \quad T \in [0, 1], 	\\
		\mfR(\mfu) & = \mfR(u, v) = \begin{pmatrix}
			u - u^3 - v - \alpha	\\
			\beta(u - v)
		\end{pmatrix},
    \end{aligned}
	\end{equation}
	The initial data is given by $\mfu_0$ is a random field and generated by i.i.d. sampling from a normal distribution and $\alpha = 0.001, \beta=1.0, \gamma = \begin{pmatrix}
		0.05 & 0	\\
		0 & 0.1
	\end{pmatrix}$. We use mesh size $128 \times 128$ for the whole problem. Computational domain is given by $[0, 6.4]\times[0, 6.4]$.
\end{frame}

\end{document}